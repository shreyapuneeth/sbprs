# -*- coding: utf-8 -*-
"""model.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15RcdaVO4asZ0lrbB8g7VnnXa-Wjxo6Cm

### Capstone : Sentiment based product recommendation system 
#### Puneeth Shivashankara
### Problem Statement:
Suppose you are working as a Machine Learning Engineer in an e-commerce company named 'Ebuss'. Ebuss has captured a huge market share in many fields, and it sells the products in various categories such as household essentials, books, personal care products, medicines, cosmetic items, beauty products, electrical appliances, kitchen and dining products and health care products.

With the advancement in technology, it is imperative for Ebuss to grow quickly in the e-commerce market to become a major leader in the market because it has to compete with the likes of Amazon, Flipkart, etc., which are already market leaders.

As a senior ML Engineer, you are asked to build a model that will improve the recommendations given to the users given their past reviews and ratings. 

In order to do this, you planned to build a sentiment-based product recommendation system, which includes the following tasks.

*Data sourcing and sentiment analysis

*Building a recommendation system

*Improving the recommendations using the sentiment analysis model

*Deploying the end-to-end project with a user interface

#### Importing the required libraries
"""

#!pip install spellchecker

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import string
import nltk
#pd.set_option('display.max_colwidth', -1)
pd.set_option('max_colwidth', -1)
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
nltk.download('punkt')
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
#!pip3 install xgboost
import xgboost as xgb
from sklearn.metrics import confusion_matrix

#!pip install pyspellchecker
from nltk import FreqDist
from matplotlib import pyplot as plt
import seaborn as sns
# %matplotlib inline

import pickle
from spellchecker import SpellChecker

from imblearn.over_sampling import SMOTE

#from nltk.stem.snowball import SnowballStemmer
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score

from google.colab import drive
drive.mount('/content/gdrive')

#Reading the file
product = pd.read_csv("/content/gdrive/MyDrive/Dataset/sample30.csv")

#checking first 5 rows
product.head(5)

#Checking last 5 rows
product.tail(5)

product.shape

product.columns

product.dtypes

product.info()

product.describe()['reviews_rating']

#Find the minimum and maximum rating given
print('Minimum rating is: %d' %(product.reviews_rating.min()))
print('Maximum rating is: %d' %(product.reviews_rating.max()))

"""The rating for the products varies from 1 to 5

#### Adding the review text and review title into a new column 'review'
"""

product["review"]= product['reviews_title'].map(str) + ' ' + product['reviews_text'].map(str)

"""#### Dropping columns reviews date ,reviews_doRecommended,reviews_didPurchase,review_usercity,review_usernanme"""

drop_list=['reviews_text','reviews_title','manufacturer','reviews_date','reviews_didPurchase','reviews_doRecommend',
          'reviews_userProvince','reviews_userCity']
product=product.drop(drop_list,axis=1)

product.head()

product.shape

Punctuation="!#$%&\()*+,-./:;<=>?@[\\]^_`{|}~"
def removePunctuation(text):
    import string
    translator = text.translate(str.maketrans("","", string.punctuation))
    return translator

def lowercase(text):
     return text.lower()

cachedStopWords = stopwords.words("english")

def remove_stopword(text):    
    tokens = word_tokenize(text)
    tokens_without_sw = [word for word in tokens if word not in cachedStopWords]    
    return  " ".join(tokens_without_sw)

def remove_singleChar(text):
    tokens = word_tokenize(text)
    res = ' '.join([word  for word in tokens if len(word)>2])
    return res

def remove_num(text):  
    tokens = word_tokenize(text)
    res = ' '.join([word  for word in tokens if not word.isdigit()])
    return res

"""### Data Cleaning"""

product['review']=product['review'].apply(lowercase)

product['review']=product['review'].apply(removePunctuation)

product['review']=product['review'].apply(remove_stopword)

product['review']=product['review'].apply(remove_singleChar)

product['review']=product['review'].apply(remove_num)

product.head()

"""spell = SpellChecker()

def spellCheck(text):
    correct_word = [spell.correction(word) for word in text.split()]
    return  " ".join(correct_word)
"""

#product['review']=product['review'].apply(spellCheck)

#from nltk.stem.snowball import SnowballStemmer

"""stemmer = SnowballStemmer("english")
def SnowballStemmer(text):
    tokens = word_tokenize(text.lower())    
    snowball_stemmed = [stemmer.stem(token) for token in tokens]
    return  " ".join(snowball_stemmed)
"""

#product['review']=product['review'].apply(SnowballStemmer)

product.to_csv("product_final.csv",index=False)

product_2 = pd.read_csv("product_final.csv").dropna()

product_2.head(1)

def plot_word_frequency(words, top_n=10):
    word_freq = FreqDist(words)
    labels = [element[0] for element in word_freq.most_common(top_n)]
    counts = [element[1] for element in word_freq.most_common(top_n)]
    plot = sns.barplot(labels, counts)
    return plot


def review_as_statement(text):
     return str(text)

document=""
document=document+ " ".join(map(str,product_2['review'].apply(review_as_statement)))

words = nltk.tokenize.word_tokenize(document)
plot_word_frequency(words, 10)

"""### Model Building

### Model: Logistic Regression
"""

vectorizer = TfidfVectorizer()
tfidf_model = vectorizer.fit_transform(product_2.review)

X_train, X_test, y_train, y_test = train_test_split(tfidf_model,product_2.user_sentiment,test_size=.7, random_state=0)

logit = LogisticRegression(C=5e1, solver='lbfgs', multi_class='multinomial', random_state=17, n_jobs=4)

logit.fit(X_train, y_train)

predict_test = logit.predict(X_test)

y_test_pred=pd.DataFrame(predict_test, columns=['Predicted'])
y_test_actual=pd.DataFrame(y_test).reset_index(drop=True)
y_test_pred['Actual']=y_test_actual.user_sentiment
y_test_pred.head()

squarer = lambda t: 1 if t=='Positive' else 0
vfunc = np.vectorize(squarer)
y_pred_copy=vfunc(predict_test)
y_test_copy=y_test.apply(lambda x:0 if x=='Negative' else 1) 
    
    
accuracy = round(accuracy_score(y_test_copy,y_pred_copy), 3)
precision = round(precision_score(y_test_copy,y_pred_copy), 3)
recall = round(recall_score(y_test_copy,y_pred_copy), 3)
f1     =round(f1_score(y_test_copy, y_pred_copy),3)
print(' A: {} / P: {} / R: {} / F1 : {}'.format(
                                    accuracy,
                                    precision,
                                    recall,
                                      f1))

confusion_matrix(y_test_copy,y_pred_copy)
pd.crosstab(y_test_copy, y_pred_copy, rownames = ['Actual'], colnames =['Predicted'], margins = True)

"""### Minority class correct pred:45%
### Class Imbalance Handling
"""

smt=SMOTE()
X_train_sm,y_train_sm=smt.fit_resample(X_train,y_train)

X_train.shape,y_train.shape,X_train_sm.shape,y_train_sm.shape

"""### Fitting the model after class balancing"""

logit.fit(X_train_sm, y_train_sm)

predict_test = logit.predict(X_test)

y_test_pred=pd.DataFrame(predict_test, columns=['Predicted'])
y_test_actual=pd.DataFrame(y_test).reset_index(drop=True)
y_test_pred['Actual']=y_test_actual.user_sentiment
y_test_pred.head()

squarer = lambda t: 1 if t=='Positive' else 0
vfunc = np.vectorize(squarer)
y_pred_copy=vfunc(predict_test)
y_test_copy=y_test.apply(lambda x:0 if x=='Negative' else 1) 
    
    
accuracy = round(accuracy_score(y_test_copy,y_pred_copy), 3)
precision = round(precision_score(y_test_copy,y_pred_copy), 3)
recall = round(recall_score(y_test_copy,y_pred_copy), 3)
f1     =round(f1_score(y_test_copy, y_pred_copy),3)
print(' A: {} / P: {} / R: {} / F1 : {}'.format(
                                    accuracy,
                                    precision,
                                    recall,
                                      f1))

confusion_matrix(y_test_copy,y_pred_copy)
pd.crosstab(y_test_copy, y_pred_copy, rownames = ['Actual'], colnames =['Predicted'], margins = True)

"""The percentage of minority class correctly predicted is very low with both imbalanced(40 percent) and balanced data set(55 percent).

#### Predicting the LR on the entire data set.
"""

predict_test = logit.predict(tfidf_model )

y_test_pred=pd.DataFrame(predict_test, columns=['Predicted'])
y_test_actual=pd.DataFrame(product_2.user_sentiment).reset_index(drop=True)
y_test_pred['Actual']=y_test_actual.user_sentiment
y_test_pred.head()

squarer = lambda t: 1 if t=='Positive' else 0
vfunc = np.vectorize(squarer)
y_pred_copy=vfunc(predict_test)
y_test_copy=product_2.user_sentiment.apply(lambda x:0 if x=='Negative' else 1) 
    
    
accuracy = round(accuracy_score(y_test_copy,y_pred_copy), 3)
precision = round(precision_score(y_test_copy,y_pred_copy), 3)
recall = round(recall_score(y_test_copy,y_pred_copy), 3)
f1     =round(f1_score(y_test_copy, y_pred_copy),3)
print(' A: {} / P: {} / R: {} / F1 : {}'.format(
                                    accuracy,
                                    precision,
                                    recall,
                                      f1))

confusion_matrix(y_test_copy,y_pred_copy)
pd.crosstab(y_test_copy, y_pred_copy, rownames = ['Actual'], colnames =['Predicted'], margins = True)

"""#### On Applying the Logistic regression after balancing the data set, on the entire table, gives a  accuracy of 92 percent.

### Predicting the entire dataset with LR model and storing the predicted value in a new column 'user_sentiment_pred'.
"""

predict_test = logit.predict(tfidf_model )

y_test_pred=pd.DataFrame(predict_test, columns=['Predicted'])
y_test_actual=pd.DataFrame(product_2.user_sentiment).reset_index(drop=True)
y_test_pred['Actual']=y_test_actual.user_sentiment
y_test_pred.head()

squarer = lambda t: 1 if t=='Positive' else 0
vfunc = np.vectorize(squarer)
y_pred_copy=vfunc(predict_test)
y_test_copy=product_2.user_sentiment.apply(lambda x:0 if x=='Negative' else 1) 
    
    
accuracy = round(accuracy_score(y_test_copy,y_pred_copy), 3)
precision = round(precision_score(y_test_copy,y_pred_copy), 3)
recall = round(recall_score(y_test_copy,y_pred_copy), 3)
f1     =round(f1_score(y_test_copy, y_pred_copy),3)
print(' A: {} / P: {} / R: {} / F1 : {}'.format(
                                    accuracy,
                                    precision,
                                    recall,
                                      f1))

confusion_matrix(y_test_copy,y_pred_copy)
pd.crosstab(y_test_copy, y_pred_copy, rownames = ['Actual'], colnames =['Predicted'], margins = True)

product_2['user_sentiment_pred']=y_test_pred['Predicted']

product_2.to_csv("prod_with_pred.csv",index=False)

"""Reading the prod_with_pred dataset and working on the data to build Recommendation system."""

ratings=pd.read_csv("prod_with_pred.csv")
ratings.head(1)

ratings = ratings.dropna()

ratings['id_2']=ratings['id']
ratings['reviews_username_2']=ratings['reviews_username']
ratings.head(1)

from sklearn.preprocessing import OrdinalEncoder
enc = OrdinalEncoder()
ratings[["id","reviews_username"]] = enc.fit_transform(ratings[["id","reviews_username"]])

def int_conv(x):
    return int(x)


ratings['id']=ratings['id'].apply(int_conv)
ratings['reviews_username']=ratings['reviews_username'].apply(int_conv)
ratings['reviews_rating']=ratings['reviews_rating'].apply(int_conv)
ratings=ratings.drop_duplicates(subset=['id', 'reviews_rating','reviews_username'],
                                       keep='first')

ratings.head(1)

ratings.nunique()

"""## Saving the ratings data set after pre processing the data set needed for recommendation system."""

ratings.to_csv("ratings.csv",index=False)

"""### Saving ratings file to be used for sentiment analysis of the 20 recommended movies.

"""

# Test and Train split of the dataset.
train, test = train_test_split(ratings, test_size=0.30, random_state=31)

train.nunique()

train.head(2)

# Pivot the train ratings' dataset into matrix format in which columns are movies and the rows are user IDs.
df_pivot = train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating'
).fillna(0)

"""# Creating dummy train & dummy test dataset
These dataset will be used for prediction 
- Dummy train will be used later for prediction of the products which has not been rated by the user. To ignore the products rated by the user, we will mark it as 0 during prediction. The products not rated by user is marked as 1 for prediction in dummy train dataset. 

- Dummy test will be used for evaluation. To evaluate, we will only make prediction on the products rated by the user. So, this is marked as 1. This is just opposite of dummy_train.
"""

# Copy the train dataset into dummy_train
dummy_train = train.copy()

dummy_train.head()

# The products not rated by user is marked as 1 for prediction. 
dummy_train['reviews_rating'] = dummy_train['reviews_rating'].apply(lambda x: 0 if x>=1 else 1)

# Convert the dummy train dataset into matrix format.
dummy_train = dummy_train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating'
).fillna(1)

dummy_train.head()

"""# **Cosine Similarity**

Cosine Similarity is a measurement that quantifies the similarity between two vectors [Which is Rating Vector in this case] 

**Adjusted Cosine**

Adjusted cosine similarity is a modified version of vector-based similarity where we incorporate the fact that different users have different ratings schemes. In other words, some users might rate items highly in general, and others might give items lower ratings as a preference. To handle this nature from rating given by user , we subtract average ratings for each user from each user's rating for different movies.

# User Similarity Matrix
# Using Cosine Similarity
"""

df_pivot.index.nunique()

from sklearn.metrics.pairwise import pairwise_distances

# Creating the User Similarity Matrix using pairwise_distance function.
user_correlation = 1 - pairwise_distances(df_pivot, metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0
print(user_correlation)

user_correlation.shape

"""# Using adjusted Cosine

Here, we are not removing the NaN values and calculating the mean only for the movies rated by the user
"""

# Create a user-movie matrix.
df_pivot = train.pivot_table(
    index='reviews_username',
    columns='id',
    values='reviews_rating'
)

"""Normalising the rating of the movie for each user around 0 mean"""

mean = np.nanmean(df_pivot, axis=1)
df_subtracted = (df_pivot.T-mean).T

df_subtracted.head()

"""# ##Finding cosine similarity"""

# Creating the User Similarity Matrix using pairwise_distance function.
user_correlation = 1 - pairwise_distances(df_subtracted.fillna(0), metric='cosine')
user_correlation[np.isnan(user_correlation)] = 0
print(user_correlation)

user_correlation.shape

"""## Prediction - User User

Doing the prediction for the users which are positively related with other users, and not the users which are negatively related as we are interested in the users which are more similar to the current users. So, ignoring the correlation for values less than 0.
"""

user_correlation[user_correlation<0]=0
user_correlation

"""Rating predicted by the user (for products rated as well as not rated) is the weighted sum of correlation with the product rating (as present in the rating dataset). """

user_predicted_ratings = np.dot(user_correlation, df_pivot.fillna(0))
user_predicted_ratings

user_predicted_ratings.shape

user_predicted_ratings

"""Since we are interested only in the products not rated by the user, we will ignore the products rated by the user by making it zero. """

user_final_rating = np.multiply(user_predicted_ratings,dummy_train)
user_final_rating.head()

user_final_rating.head()

"""### Saving the Model"""

file_name = "Recommendation.pkl"
pickle.dump(user_final_rating, open(file_name, "wb"))

"""# Finding the top 20 recommendation for the *user*"""

# Take the user ID as input.
user_input = int(input("Enter your user name"))
print(user_input)

user_final_rating.head()

d = user_final_rating.loc[user_input].sort_values(ascending=False)[0:20]
d

#Mapping with product name
pdt_mapping = ratings
pdt_mapping.head()

d = pd.merge(d,pdt_mapping,left_on='id',right_on='id', how = 'left')
d.head()

train_new = pd.merge(train,pdt_mapping,left_on='id',right_on='id',how='left')



"""### Evaluation - User User

Evaluation will we same as you have seen above for the prediction. The only difference being, you will evaluate for the product already rated by the user insead of predicting it for the product not rated by the user.
"""

# Find out the common users of test and train dataset.
common = test[test.reviews_username.isin(train.reviews_username)]
common.shape

common.head()

# convert into the user-movie matrix.
common_user_based_matrix = common.pivot_table(index='reviews_username', columns='id', values='reviews_rating')

common_user_based_matrix.head()

# Convert the user_correlation matrix into dataframe.
user_correlation_df = pd.DataFrame(user_correlation)

user_correlation_df['reviews_username'] = df_subtracted.index

user_correlation_df.set_index('reviews_username',inplace=True)
user_correlation_df.head()

list_name = common.reviews_username.tolist()

user_correlation_df.columns = df_subtracted.index.tolist()


user_correlation_df_1 =  user_correlation_df[user_correlation_df.index.isin(list_name)]

user_correlation_df_2 = user_correlation_df_1.T[user_correlation_df_1.T.index.isin(list_name)]

user_correlation_df_3 = user_correlation_df_2.T

user_correlation_df_3.head()

user_correlation_df_3[user_correlation_df_3<0]=0

common_user_predicted_ratings = np.dot(user_correlation_df_3, common_user_based_matrix.fillna(0))
common_user_predicted_ratings

dummy_test = common.copy()

dummy_test['reviews_rating'] = dummy_test['reviews_rating'].apply(lambda x: 1 if x>=1 else 0)

dummy_test = dummy_test.pivot_table(index='reviews_username', columns='id', values='reviews_rating').fillna(0)

dummy_test.shape

common_user_based_matrix.head()

common_user_predicted_ratings = np.multiply(common_user_predicted_ratings,dummy_test)

common_user_predicted_ratings.head()

"""### The Rating file contains the predicted sentiment based on LOGISTIC REGRESSION .The recommendation.pkl file contains the model for recommendation based on user id.The recommended system recommend 20 movies id, and based on that top 5 are selected from ratings data frame on the basis of sentiment."""

import pickle
import pandas as pd

file_name = "Recommendation.pkl"
Recommendation_loaded = pickle.load(open(file_name, "rb"))

ratings=pd.read_csv('ratings.csv')

ratings.head(1)

# Take the user ID as input
user_input = int(input("Enter your user name"))
print(user_input)

# Recommending the Top 20 products to the user.
d = Recommendation_loaded.loc[user_input].sort_values(ascending=False)[0:20]
d

list = d.to_frame().reset_index().id.to_list()
print(list)
#Using the id , filtering all the movies with same id.
ratings_top=ratings[ratings['id'].apply(lambda x:x in list)]
ratings_top.head(1)

ratings_top.user_sentiment_pred=ratings_top.user_sentiment_pred.apply(lambda x:0 if x=='Negative' else 1)

"""#### The count method , calculated the sentiment in terms of percentage , and then top 5 movies id are selected based on sentiment"""

list_perc=[]
def count(df):
  for x in list:
    list2=[x]
    dummy_frame=ratings_top[ratings_top.id.apply(lambda x:x  in list2)]
    deno=len(dummy_frame)
    num=dummy_frame.user_sentiment_pred.sum()
    list_perc.append(round(num/deno,3))

count(ratings_top)
list_perc

temp=d.to_frame()
temp['Sentiment_percent']=list_perc

top5_list = temp.Sentiment_percent.sort_values(ascending=False)[0:5].to_frame().reset_index().id.to_list()
top5_list

"""## PRINTING THE 5 RECOMMENDED PRODUCTS BASED ON SENTIMENT"""

product_recommended=set(ratings_top[ratings_top['id'].apply(lambda x:x in top5_list)].name.tolist())
product_recommended